{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"yoloimport.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"14vDeIZ8W6W4N1-0feE7r583_pVb1tVHv","authorship_tag":"ABX9TyPh5OT8r/lziCW5D1Pt2N5+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cL7xP_uSjibZ","colab_type":"text"},"source":["# Importing the libraries"]},{"cell_type":"code","metadata":{"id":"cp0GBOZNix0m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592402930860,"user_tz":-330,"elapsed":3481,"user":{"displayName":"Veeraraghavan N","photoUrl":"","userId":"14132516893778461865"}},"outputId":"5620264e-0f90-4f03-ad7b-66e8f367d0e1"},"source":["import argparse\n","import os\n","import numpy as np\n","from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n","from keras.layers.merge import add, concatenate\n","from keras.models import Model\n","import struct\n","import cv2\n","import tensorflow as tf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"JY_J8i1nj-ab","colab_type":"text"},"source":["# Using the open source code provided by experiencor in the keras-yolo3 project \n","\n","https://raw.githubusercontent.com/experiencor/keras-yolo3/master/yolo3_one_file_to_detect_them_all.py"]},{"cell_type":"code","metadata":{"id":"VyvLPsJ0j8Jg","colab_type":"code","colab":{}},"source":["def _conv_block(inp, convs, skip=True):\n","\tx = inp\n","\tcount = 0\n","\tfor conv in convs:\n","\t\tif count == (len(convs) - 2) and skip:\n","\t\t\tskip_connection = x\n","\t\tcount += 1\n","\t\tif conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n","\t\tx = Conv2D(conv['filter'],\n","\t\t\t\t   conv['kernel'],\n","\t\t\t\t   strides=conv['stride'],\n","\t\t\t\t   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n","\t\t\t\t   name='conv_' + str(conv['layer_idx']),\n","\t\t\t\t   use_bias=False if conv['bnorm'] else True)(x)\n","\t\tif conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n","\t\tif conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n","\treturn add([skip_connection, x]) if skip else x\n","\n","def make_yolov3_model():\n","\tinput_image = Input(shape=(None, None, 3))\n","\t# Layer  0 => 4\n","\tx = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n","\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n","\t\t\t\t\t\t\t\t  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n","\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n","\t# Layer  5 => 8\n","\tx = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n","\t\t\t\t\t\t{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n","\t# Layer  9 => 11\n","\tx = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n","\t# Layer 12 => 15\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n","\t\t\t\t\t\t{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n","\t# Layer 16 => 36\n","\tfor i in range(7):\n","\t\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n","\t\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n","\tskip_36 = x\n","\t# Layer 37 => 40\n","\tx = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n","\t# Layer 41 => 61\n","\tfor i in range(7):\n","\t\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n","\t\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n","\tskip_61 = x\n","\t# Layer 62 => 65\n","\tx = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n","\t# Layer 66 => 74\n","\tfor i in range(3):\n","\t\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n","\t\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n","\t# Layer 75 => 79\n","\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n","\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n","\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n","\t# Layer 80 => 82\n","\tyolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n","\t\t\t\t\t\t\t  {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n","\t# Layer 83 => 86\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n","\tx = UpSampling2D(2)(x)\n","\tx = concatenate([x, skip_61])\n","\t# Layer 87 => 91\n","\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n","\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n","\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n","\t# Layer 92 => 94\n","\tyolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n","\t\t\t\t\t\t\t  {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n","\t# Layer 95 => 98\n","\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n","\tx = UpSampling2D(2)(x)\n","\tx = concatenate([x, skip_36])\n","\t# Layer 99 => 106\n","\tyolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n","\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n","\t\t\t\t\t\t\t   {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n","\t\t\t\t\t\t\t   {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n","\t\t\t\t\t\t\t   {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n","\tmodel = Model(input_image, [yolo_82, yolo_94, yolo_106])\n","\treturn model\n","\n","class WeightReader:\n","\tdef __init__(self, weight_file):\n","\t\twith open(weight_file, 'rb') as w_f:\n","\t\t\tmajor,\t= struct.unpack('i', w_f.read(4))\n","\t\t\tminor,\t= struct.unpack('i', w_f.read(4))\n","\t\t\trevision, = struct.unpack('i', w_f.read(4))\n","\t\t\tif (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n","\t\t\t\tw_f.read(8)\n","\t\t\telse:\n","\t\t\t\tw_f.read(4)\n","\t\t\ttranspose = (major > 1000) or (minor > 1000)\n","\t\t\tbinary = w_f.read()\n","\t\tself.offset = 0\n","\t\tself.all_weights = np.frombuffer(binary, dtype='float32')\n"," \n","\tdef read_bytes(self, size):\n","\t\tself.offset = self.offset + size\n","\t\treturn self.all_weights[self.offset-size:self.offset]\n"," \n","\tdef load_weights(self, model):\n","\t\tfor i in range(106):\n","\t\t\ttry:\n","\t\t\t\tconv_layer = model.get_layer('conv_' + str(i))\n","\t\t\t\tprint(\"loading weights of convolution #\" + str(i))\n","\t\t\t\tif i not in [81, 93, 105]:\n","\t\t\t\t\tnorm_layer = model.get_layer('bnorm_' + str(i))\n","\t\t\t\t\tsize = np.prod(norm_layer.get_weights()[0].shape)\n","\t\t\t\t\tbeta  = self.read_bytes(size) # bias\n","\t\t\t\t\tgamma = self.read_bytes(size) # scale\n","\t\t\t\t\tmean  = self.read_bytes(size) # mean\n","\t\t\t\t\tvar   = self.read_bytes(size) # variance\n","\t\t\t\t\tweights = norm_layer.set_weights([gamma, beta, mean, var])\n","\t\t\t\tif len(conv_layer.get_weights()) > 1:\n","\t\t\t\t\tbias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n","\t\t\t\t\tkernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n","\t\t\t\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n","\t\t\t\t\tkernel = kernel.transpose([2,3,1,0])\n","\t\t\t\t\tconv_layer.set_weights([kernel, bias])\n","\t\t\t\telse:\n","\t\t\t\t\tkernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n","\t\t\t\t\tkernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n","\t\t\t\t\tkernel = kernel.transpose([2,3,1,0])\n","\t\t\t\t\tconv_layer.set_weights([kernel])\n","\t\t\texcept ValueError:\n","\t\t\t\tprint(\"no convolution #\" + str(i))\n"," \n","\tdef reset(self):\n","\t\tself.offset = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51LKTlaMmKcr","colab_type":"text"},"source":["# Creating and compiling the pretrained model using the above functions"]},{"cell_type":"code","metadata":{"id":"gf2DxNhNmRLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592403200461,"user_tz":-330,"elapsed":7672,"user":{"displayName":"Veeraraghavan N","photoUrl":"","userId":"14132516893778461865"}},"outputId":"581384e1-23d5-4217-f5c7-0d38cb47cf9b"},"source":["#creating model object\n","odm = make_yolov3_model()\n","#loading the weights into the model\n","weights = WeightReader('/content/drive/My Drive/ODyolo/yolov3.weights')\n","#set the pretrained weights to the model\n","weights.load_weights(odm)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading weights of convolution #0\n","loading weights of convolution #1\n","loading weights of convolution #2\n","loading weights of convolution #3\n","no convolution #4\n","loading weights of convolution #5\n","loading weights of convolution #6\n","loading weights of convolution #7\n","no convolution #8\n","loading weights of convolution #9\n","loading weights of convolution #10\n","no convolution #11\n","loading weights of convolution #12\n","loading weights of convolution #13\n","loading weights of convolution #14\n","no convolution #15\n","loading weights of convolution #16\n","loading weights of convolution #17\n","no convolution #18\n","loading weights of convolution #19\n","loading weights of convolution #20\n","no convolution #21\n","loading weights of convolution #22\n","loading weights of convolution #23\n","no convolution #24\n","loading weights of convolution #25\n","loading weights of convolution #26\n","no convolution #27\n","loading weights of convolution #28\n","loading weights of convolution #29\n","no convolution #30\n","loading weights of convolution #31\n","loading weights of convolution #32\n","no convolution #33\n","loading weights of convolution #34\n","loading weights of convolution #35\n","no convolution #36\n","loading weights of convolution #37\n","loading weights of convolution #38\n","loading weights of convolution #39\n","no convolution #40\n","loading weights of convolution #41\n","loading weights of convolution #42\n","no convolution #43\n","loading weights of convolution #44\n","loading weights of convolution #45\n","no convolution #46\n","loading weights of convolution #47\n","loading weights of convolution #48\n","no convolution #49\n","loading weights of convolution #50\n","loading weights of convolution #51\n","no convolution #52\n","loading weights of convolution #53\n","loading weights of convolution #54\n","no convolution #55\n","loading weights of convolution #56\n","loading weights of convolution #57\n","no convolution #58\n","loading weights of convolution #59\n","loading weights of convolution #60\n","no convolution #61\n","loading weights of convolution #62\n","loading weights of convolution #63\n","loading weights of convolution #64\n","no convolution #65\n","loading weights of convolution #66\n","loading weights of convolution #67\n","no convolution #68\n","loading weights of convolution #69\n","loading weights of convolution #70\n","no convolution #71\n","loading weights of convolution #72\n","loading weights of convolution #73\n","no convolution #74\n","loading weights of convolution #75\n","loading weights of convolution #76\n","loading weights of convolution #77\n","loading weights of convolution #78\n","loading weights of convolution #79\n","loading weights of convolution #80\n","loading weights of convolution #81\n","no convolution #82\n","no convolution #83\n","loading weights of convolution #84\n","no convolution #85\n","no convolution #86\n","loading weights of convolution #87\n","loading weights of convolution #88\n","loading weights of convolution #89\n","loading weights of convolution #90\n","loading weights of convolution #91\n","loading weights of convolution #92\n","loading weights of convolution #93\n","no convolution #94\n","no convolution #95\n","loading weights of convolution #96\n","no convolution #97\n","no convolution #98\n","loading weights of convolution #99\n","loading weights of convolution #100\n","loading weights of convolution #101\n","loading weights of convolution #102\n","loading weights of convolution #103\n","loading weights of convolution #104\n","loading weights of convolution #105\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PEE7xBBNoijS","colab_type":"text"},"source":["# Saving the model for future use"]},{"cell_type":"code","metadata":{"id":"S1oIHDUmonxE","colab_type":"code","colab":{}},"source":["#saving this model for further use\n","odm.save('/content/drive/My Drive/ODyolo/odm.h5')"],"execution_count":null,"outputs":[]}]}